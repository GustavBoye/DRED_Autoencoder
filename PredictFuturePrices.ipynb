{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa8cZta6uUK8Z2x7QBsmIi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavBoye/DRED_Autoencoder/blob/main/PredictFuturePrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eqdPf9L7RKh",
        "outputId": "52919973-6549-4fcc-e176-babd3739a5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['IAG', 'DRD', 'GOLD', 'NEM', 'GLD']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['AEM', 'PAAS', 'AU', 'KGC', 'WPM']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['AG', 'NG', 'HMY', 'GMAB', 'NMM']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['CSIQ', 'SEDG', 'FSLR', 'ENPH', 'JKS']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['RUN', 'NEE', 'CWEN', 'LNTH', 'MSFT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['NVDA', 'AMD', 'INTC', 'GOOG', 'META']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['TSM', 'INTU', 'UMC', 'MU', 'ORCL']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['ASML', 'DELL', 'TXN', 'QRVO', 'BIDU']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['TSLA', 'ALB', 'LTBR', 'CAT', 'BWXT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['BA', 'SIEGY', 'SQM', 'MA', 'MCD']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['AVXL', 'SRTS', 'HDSN', 'CCJ', 'AMZN']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['WELL', 'MPLX', 'TSE', 'ELD', 'UTHR']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['ABT', 'JNJ', 'BABA', 'JPM', 'SB']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['TER', 'ARKQ', 'BLX', 'NRG', 'LIT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "DAYS = 10\n",
        "HISTORY = 75\n",
        "\n",
        "\n",
        "# Helper function for normalization\n",
        "def normalize_min_max(data):\n",
        "    return (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n",
        "\n",
        "# Define input tickers\n",
        "input_tickers = [\n",
        "    'IAG', 'DRD', 'GOLD', 'NEM', 'GLD', 'AEM', 'PAAS', 'AU', 'KGC', 'WPM', 'AG', 'NG', 'HMY', 'GMAB', 'NMM', 'CSIQ',\n",
        "    'SEDG', 'FSLR', 'ENPH', 'JKS', 'RUN', 'NEE', 'CWEN', 'LNTH',\n",
        "    'MSFT', 'NVDA', 'AMD', 'INTC', 'GOOG', 'META', 'TSM', 'INTU', 'UMC', 'MU', 'ORCL', 'ASML', 'DELL', 'TXN', 'QRVO', 'BIDU',\n",
        "    'TSLA', 'ALB', 'LTBR', 'CAT', 'BWXT', 'BA', 'SIEGY', 'SQM', 'MA', 'MCD', 'AVXL', 'SRTS',\n",
        "    'HDSN', 'CCJ', 'AMZN', 'WELL', 'MPLX', 'TSE', 'ELD', 'UTHR', 'ABT', 'JNJ', 'BABA', 'JPM', 'SB', 'TER', 'ARKQ', 'BLX', 'NRG', 'LIT', 'FLXS', 'BAESY', 'STE', 'IBM', 'KO', 'PEP', 'GS', 'V'\n",
        "]\n",
        "\n",
        "import time\n",
        "import pandas as pd # Import pandas here\n",
        "import yfinance as yf\n",
        "\n",
        "def batch_download(tickers, start, end, interval, batch_size=5):\n",
        "    all_data = {}\n",
        "    for i in range(0, len(tickers), batch_size):\n",
        "        batch_tickers = tickers[i:i + batch_size]\n",
        "        print(f\"Downloading data for: {batch_tickers}\")\n",
        "\n",
        "        # Try to download the data for this batch\n",
        "        try:\n",
        "            data = yf.download(batch_tickers, start=start, end=end, interval=interval)['Close']\n",
        "            all_data.update(data.to_dict())\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading data for {batch_tickers}: {e}\")\n",
        "            time.sleep(0.4)  # Wait for a bit before retrying if an error occurs\n",
        "\n",
        "        time.sleep(0.5)  # Sleep between batches to avoid hitting rate limits\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Fetch stock data in batches\n",
        "data = batch_download(input_tickers, start=\"2017-01-01\", end=\"2025-02-16\", interval=\"1d\")\n",
        "data_volume = batch_download(input_tickers, start=\"2017-01-01\", end=\"2025-02-16\", interval=\"1d\")\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "scaler_vol = MinMaxScaler()\n",
        "data_scaled_vol = scaler_vol.fit_transform(data_volume)\n",
        "\n",
        "\n",
        "# Function to create supervised learning dataset\n",
        "def create_dataset(data, vol, time_steps=0, future_steps=0):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps - future_steps):\n",
        "\n",
        "        data_to_insert_a = normalize_min_max(data[i:i + time_steps])\n",
        "        data_to_insert_b = normalize_min_max(vol[i:i + time_steps])\n",
        "        data_to_insert = np.concatenate([data_to_insert_a, data_to_insert_b], axis=0)\n",
        "        X.append(data_to_insert)\n",
        "\n",
        "        future_prices = data[i + time_steps:i + time_steps + future_steps]\n",
        "        current_price = data[i + time_steps - 1]\n",
        "        future_average_price = np.mean(future_prices, axis=0)\n",
        "        target = future_average_price - current_price\n",
        "        y.append(target)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Function to create inverted dataset\n",
        "def create_inverted_dataset(data, vol, time_steps=0, future_steps=0):\n",
        "    inverted_data = 1-data\n",
        "    X, y = [], []\n",
        "    for i in range(len(inverted_data) - time_steps - future_steps):\n",
        "\n",
        "        data_to_insert_a = normalize_min_max(inverted_data[i:i + time_steps])\n",
        "        data_to_insert_b = normalize_min_max(vol[i:i + time_steps])\n",
        "        data_to_insert = np.concatenate([data_to_insert_a, data_to_insert_b], axis=0)\n",
        "        X.append(data_to_insert)\n",
        "\n",
        "        future_prices = inverted_data[i + time_steps:i + time_steps + future_steps]\n",
        "        current_price = inverted_data[i + time_steps - 1]\n",
        "        future_average_price = np.mean(future_prices, axis=0)\n",
        "        target = future_average_price - current_price\n",
        "        y.append(target)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare dataset (200-day version)\n",
        "X_s, y_s = create_dataset(data_scaled, data_scaled_vol, time_steps=HISTORY, future_steps=DAYS)\n",
        "X_inv_s, y_inv_s = create_inverted_dataset(data_scaled, data_scaled_vol, time_steps=HISTORY, future_steps=DAYS)\n",
        "X_combined_s = np.concatenate((X_s, X_inv_s), axis=0)\n",
        "y_combined_s = np.concatenate((y_s, y_inv_s), axis=0)\n",
        "\n",
        "def build_model(shape1, shape2):\n",
        "    input_layer = tf.keras.layers.Input(shape=(shape1, shape2))\n",
        "    x = tf.keras.layers.SpatialDropout1D(0.3)(input_layer)\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.7)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(128, 3, strides=1, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "    x = tf.keras.layers.Dropout(0.7)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.7)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(128, 3, strides=1, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "    x = tf.keras.layers.Dropout(0.8)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    dense1 = tf.keras.layers.Dense(256, activation='gelu')(x)\n",
        "    dense1 = tf.keras.layers.Dropout(0.6)(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(256, activation='gelu')(dense1)\n",
        "    dense = dense1+dense2\n",
        "    dense = tf.keras.layers.Dropout(0.4)(dense)\n",
        "\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(input_tickers))(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "def build_model_2(shape1, shape2):\n",
        "    input_layer = tf.keras.layers.Input(shape=(shape1, shape2))\n",
        "    x = tf.keras.layers.SpatialDropout1D(0.2)(input_layer)\n",
        "    x = tf.keras.layers.Conv1D(16, 5, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(16, 1, strides=1, padding=\"same\")(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(16, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(16, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    dense1 = tf.keras.layers.Dense(16, activation='gelu')(x)\n",
        "    dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(16, activation='gelu')(dense1)\n",
        "    dense = dense1+dense2\n",
        "\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(input_tickers))(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "def build_model3(shape1, shape2):\n",
        "    input_layer = tf.keras.layers.Input(shape=(shape1, shape2))\n",
        "    x = tf.keras.layers.SpatialDropout1D(0.4)(input_layer)\n",
        "    x = tf.keras.layers.Conv1D(32, 5, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(32, 1, strides=1, padding=\"same\")(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(32, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "\n",
        "    x = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)  # âœ… Add Transformer attention\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(64, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    dense1 = tf.keras.layers.Dense(128, activation='gelu')(x)\n",
        "    dense1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(128, activation='gelu')(dense1)\n",
        "    dense = dense1+dense2\n",
        "\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(input_tickers))(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Training setup\n",
        "num_models = 2\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=64, min_delta=1e-4, restore_best_weights=True, mode='min', verbose=1)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=16, verbose=1)\n",
        "\n",
        "import random\n",
        "# Train the ensemble (200-day version)\n",
        "ensemble = []\n",
        "for i in range(num_models):\n",
        "    print(f\"Training model {i+1}/{num_models} for {DAYS}-day prediction...\")\n",
        "    model = build_model(X_combined_s.shape[1], X_combined_s.shape[2])\n",
        "    optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-5)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    model.fit(X_combined_s, y_combined_s, epochs=50, batch_size=8, validation_split=0.3, verbose=1, callbacks=[early_stopping, lr_scheduler])\n",
        "    ensemble.append(model)\n",
        "\n",
        "# Predictions (200-day version)\n",
        "predictions = {ticker: [] for ticker in input_tickers}\n",
        "for idx, ticker in enumerate(input_tickers):\n",
        "    predictions[ticker] = np.mean([m(X_combined_s[:1]).numpy()[:, idx] for m in ensemble], axis=0)\n",
        "\n",
        "# Plot predictions\n",
        "fig, ax = plt.subplots(figsize=(18, 10))\n",
        "tickers_range = np.arange(len(input_tickers))\n",
        "pred_days = np.array([predictions[ticker] for ticker in input_tickers]).flatten()\n",
        "ax.bar(tickers_range, pred_days, color='blue', width=0.6, label='200-Day Prediction')\n",
        "\n",
        "ax.set_title(\"200-Day Predictions for All Tickers\", fontsize=16)\n",
        "ax.set_xlabel(\"Tickers\", fontsize=14)\n",
        "ax.set_ylabel(\"Predicted Change\", fontsize=14)\n",
        "ax.set_xticks(tickers_range)\n",
        "ax.set_xticklabels(input_tickers, rotation=45, fontsize=10)\n",
        "ax.legend(title=\"Prediction Length\", fontsize=10)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Convert predictions dictionary to DataFrame\n",
        "predictions_df = pd.DataFrame.from_dict(predictions, orient='index', columns=['{DAYS}-Day Prediction'])\n",
        "\n",
        "# Save as CSV\n",
        "predictions_df.to_csv(f\"predictions_{DAYS}.csv\")\n",
        "\n",
        "# Download in Colab\n",
        "from google.colab import files\n",
        "files.download(f\"predictions_{DAYS}.csv\")"
      ]
    }
  ]
}