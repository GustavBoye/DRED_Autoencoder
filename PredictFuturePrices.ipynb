{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI5xv73IY4wyazhSJQOuyr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavBoye/DRED_Autoencoder/blob/main/PredictFuturePrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "9eqdPf9L7RKh",
        "outputId": "a1ca60b6-09a2-4cd5-e380-a4d542b6ee6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['IAG', 'DRD', 'GOLD', 'NEM', 'GLD']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "ERROR:yfinance:\n",
            "5 Failed downloads:\n",
            "ERROR:yfinance:['IAG', 'NEM', 'GOLD', 'DRD', 'GLD']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['AEM', 'PAAS', 'AU', 'KGC', 'WPM']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  5 of 5 completed\n",
            "ERROR:yfinance:\n",
            "5 Failed downloads:\n",
            "ERROR:yfinance:['AU', 'WPM', 'PAAS', 'KGC', 'AEM']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for: ['AG', 'NG', 'GMAB', 'SEDG', 'FSLR']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-93945039b918>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Fetch stock data in batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2017-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2025-02-16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mdata_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2017-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2025-02-16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-93945039b918>\u001b[0m in \u001b[0;36mbatch_download\u001b[0;34m(tickers, start, end, interval, batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Try to download the data for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mIndentationContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Exiting {func.__name__}()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/multi.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, repair, keepna, progress, period, interval, prepost, proxy, rounding, timeout, session, multi_level_index)\u001b[0m\n\u001b[1;32m    155\u001b[0m                                    rounding=rounding, timeout=timeout)\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DFS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0m_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;31m# download synchronously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "DAYS = 10\n",
        "HISTORY = 75\n",
        "\n",
        "\n",
        "# Helper function for normalization\n",
        "def normalize_min_max(data):\n",
        "    return (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n",
        "\n",
        "# Define input tickers\n",
        "input_tickers = [\n",
        "    'IAG', 'DRD', 'GOLD', 'NEM', 'GLD', 'AEM', 'PAAS', 'AU', 'KGC', 'WPM', 'AG', 'NG', 'GMAB',\n",
        "    'SEDG', 'FSLR', 'ENPH', 'JKS', 'RUN', 'NEE', 'CWEN', 'LNTH',\n",
        "    'MSFT', 'NVDA', 'AMD', 'INTC', 'GOOG', 'META', 'TSM', 'INTU', 'UMC', 'MU', 'ORCL', 'ASML', 'DELL', 'TXN', 'QRVO', 'BIDU',\n",
        "    'TSLA', 'ALB', 'LTBR', 'CAT', 'BWXT', 'BA', 'SIEGY', 'SQM', 'MA', 'MCD', 'AVXL', 'SRTS',\n",
        "    'HDSN', 'CCJ', 'AMZN', 'WELL', 'MPLX', 'TSE', 'ELD', 'UTHR', 'ABT', 'JNJ', 'BABA', 'JPM', 'SB', 'TER', 'ARKQ', 'BLX', 'NRG', 'LIT', 'FLXS', 'BAESY', 'STE', 'IBM', 'KO', 'PEP', 'GS', 'V'\n",
        "]\n",
        "\n",
        "import time\n",
        "import pandas as pd # Import pandas here\n",
        "import yfinance as yf\n",
        "\n",
        "def batch_download(tickers, start, end, interval, batch_size=5):\n",
        "    all_data = {}\n",
        "    for i in range(0, len(tickers), batch_size):\n",
        "        batch_tickers = tickers[i:i + batch_size]\n",
        "        print(f\"Downloading data for: {batch_tickers}\")\n",
        "\n",
        "        # Try to download the data for this batch\n",
        "        try:\n",
        "            data = yf.download(batch_tickers, start=start, end=end, interval=interval)['Close']\n",
        "            all_data.update(data.to_dict())\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading data for {batch_tickers}: {e}\")\n",
        "            time.sleep(4)  # Wait for a bit before retrying if an error occurs\n",
        "\n",
        "        time.sleep(5)  # Sleep between batches to avoid hitting rate limits\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Fetch stock data in batches\n",
        "data = batch_download(input_tickers, start=\"2017-01-01\", end=\"2025-02-16\", interval=\"1d\")\n",
        "data_volume = batch_download(input_tickers, start=\"2017-01-01\", end=\"2025-02-16\", interval=\"1d\")\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "scaler_vol = MinMaxScaler()\n",
        "data_scaled_vol = scaler_vol.fit_transform(data_volume)\n",
        "\n",
        "\n",
        "# Function to create supervised learning dataset\n",
        "def create_dataset(data, vol, time_steps=0, future_steps=0):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps - future_steps):\n",
        "\n",
        "        data_to_insert_a = normalize_min_max(data[i:i + time_steps])\n",
        "        data_to_insert_b = normalize_min_max(vol[i:i + time_steps])\n",
        "        data_to_insert = np.concatenate([data_to_insert_a, data_to_insert_b], axis=0)\n",
        "        X.append(data_to_insert)\n",
        "\n",
        "        future_prices = data[i + time_steps:i + time_steps + future_steps]\n",
        "        current_price = data[i + time_steps - 1]\n",
        "        future_average_price = np.mean(future_prices, axis=0)\n",
        "        target = future_average_price - current_price\n",
        "        y.append(target)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Function to create inverted dataset\n",
        "def create_inverted_dataset(data, vol, time_steps=0, future_steps=0):\n",
        "    inverted_data = 1-data\n",
        "    X, y = [], []\n",
        "    for i in range(len(inverted_data) - time_steps - future_steps):\n",
        "\n",
        "        data_to_insert_a = normalize_min_max(inverted_data[i:i + time_steps])\n",
        "        data_to_insert_b = normalize_min_max(vol[i:i + time_steps])\n",
        "        data_to_insert = np.concatenate([data_to_insert_a, data_to_insert_b], axis=0)\n",
        "        X.append(data_to_insert)\n",
        "\n",
        "        future_prices = inverted_data[i + time_steps:i + time_steps + future_steps]\n",
        "        current_price = inverted_data[i + time_steps - 1]\n",
        "        future_average_price = np.mean(future_prices, axis=0)\n",
        "        target = future_average_price - current_price\n",
        "        y.append(target)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepare dataset (200-day version)\n",
        "X_s, y_s = create_dataset(data_scaled, data_scaled_vol, time_steps=HISTORY, future_steps=DAYS)\n",
        "X_inv_s, y_inv_s = create_inverted_dataset(data_scaled, data_scaled_vol, time_steps=HISTORY, future_steps=DAYS)\n",
        "X_combined_s = np.concatenate((X_s, X_inv_s), axis=0)\n",
        "y_combined_s = np.concatenate((y_s, y_inv_s), axis=0)\n",
        "\n",
        "def build_model(shape1, shape2):\n",
        "    input_layer = tf.keras.layers.Input(shape=(shape1, shape2))\n",
        "    x = tf.keras.layers.SpatialDropout1D(0.3)(input_layer)\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.7)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(128, 3, strides=1, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "    x = tf.keras.layers.Dropout(0.7)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.7)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(128, 3, strides=1, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "    x = tf.keras.layers.Dropout(0.8)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(128, 3, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.002))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    dense1 = tf.keras.layers.Dense(256, activation='gelu')(x)\n",
        "    dense1 = tf.keras.layers.Dropout(0.6)(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(256, activation='gelu')(dense1)\n",
        "    dense = dense1+dense2\n",
        "    dense = tf.keras.layers.Dropout(0.4)(dense)\n",
        "\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(input_tickers))(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "def build_model_2(shape1, shape2):\n",
        "    input_layer = tf.keras.layers.Input(shape=(shape1, shape2))\n",
        "    x = tf.keras.layers.SpatialDropout1D(0.2)(input_layer)\n",
        "    x = tf.keras.layers.Conv1D(16, 5, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(16, 1, strides=1, padding=\"same\")(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(16, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(16, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    dense1 = tf.keras.layers.Dense(16, activation='gelu')(x)\n",
        "    dense1 = tf.keras.layers.Dropout(0.1)(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(16, activation='gelu')(dense1)\n",
        "    dense = dense1+dense2\n",
        "\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(input_tickers))(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "def build_model3(shape1, shape2):\n",
        "    input_layer = tf.keras.layers.Input(shape=(shape1, shape2))\n",
        "    x = tf.keras.layers.SpatialDropout1D(0.4)(input_layer)\n",
        "    x = tf.keras.layers.Conv1D(32, 5, strides=2, padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    x1 = tf.keras.layers.Conv1D(32, 1, strides=1, padding=\"same\")(x)\n",
        "    x1 = tf.keras.layers.Activation('gelu')(x1)\n",
        "    x = x + x1  # Residual connection\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(32, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "\n",
        "    x = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)  # ✅ Add Transformer attention\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(64, 5, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('gelu')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    dense1 = tf.keras.layers.Dense(128, activation='gelu')(x)\n",
        "    dense1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
        "    dense2 = tf.keras.layers.Dense(128, activation='gelu')(dense1)\n",
        "    dense = dense1+dense2\n",
        "\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(input_tickers))(dense)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Training setup\n",
        "num_models = 2\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=64, min_delta=1e-4, restore_best_weights=True, mode='min', verbose=1)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=16, verbose=1)\n",
        "\n",
        "import random\n",
        "# Train the ensemble (200-day version)\n",
        "ensemble = []\n",
        "for i in range(num_models):\n",
        "    print(f\"Training model {i+1}/{num_models} for {DAYS}-day prediction...\")\n",
        "    model = build_model(X_combined_s.shape[1], X_combined_s.shape[2])\n",
        "    optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-5)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    model.fit(X_combined_s, y_combined_s, epochs=50, batch_size=8, validation_split=0.3, verbose=1, callbacks=[early_stopping, lr_scheduler])\n",
        "    ensemble.append(model)\n",
        "\n",
        "# Predictions (200-day version)\n",
        "predictions = {ticker: [] for ticker in input_tickers}\n",
        "for idx, ticker in enumerate(input_tickers):\n",
        "    predictions[ticker] = np.mean([m(X_combined_s[:1]).numpy()[:, idx] for m in ensemble], axis=0)\n",
        "\n",
        "# Plot predictions\n",
        "fig, ax = plt.subplots(figsize=(18, 10))\n",
        "tickers_range = np.arange(len(input_tickers))\n",
        "pred_days = np.array([predictions[ticker] for ticker in input_tickers]).flatten()\n",
        "ax.bar(tickers_range, pred_days, color='blue', width=0.6, label='200-Day Prediction')\n",
        "\n",
        "ax.set_title(\"200-Day Predictions for All Tickers\", fontsize=16)\n",
        "ax.set_xlabel(\"Tickers\", fontsize=14)\n",
        "ax.set_ylabel(\"Predicted Change\", fontsize=14)\n",
        "ax.set_xticks(tickers_range)\n",
        "ax.set_xticklabels(input_tickers, rotation=45, fontsize=10)\n",
        "ax.legend(title=\"Prediction Length\", fontsize=10)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Convert predictions dictionary to DataFrame\n",
        "predictions_df = pd.DataFrame.from_dict(predictions, orient='index', columns=['{DAYS}-Day Prediction'])\n",
        "\n",
        "# Save as CSV\n",
        "predictions_df.to_csv(f\"predictions_{DAYS}.csv\")\n",
        "\n",
        "# Download in Colab\n",
        "from google.colab import files\n",
        "files.download(f\"predictions_{DAYS}.csv\")"
      ]
    }
  ]
}