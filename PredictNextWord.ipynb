{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK8SSN4daIGyYdPZoPYYO5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavBoye/DRED_Autoencoder/blob/main/PredictNextWord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization, Add, Input, LeakyReLU\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load text from a file\n",
        "with open(\"dataset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    conversation_text = file.read()\n",
        "\n",
        "print(f\"Text length: {len(conversation_text)}\")\n",
        "\n",
        "# 📌 Preprocess text: Convert to lowercase and remove unwanted characters\n",
        "text = conversation_text.lower()\n",
        "text = ''.join(c for c in text if c.isalnum() or c in \" .,!?\")\n",
        "\n",
        "# 📌 Tokenize the text: Convert words to tokens\n",
        "tokenizer = Tokenizer(char_level=False)  # Use word-level tokenization\n",
        "tokenizer.fit_on_texts([text])\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# 📌 Prepare training data (sequence of tokens → next token)\n",
        "SEQ_LENGTH = 128  # Shorter sequences for quick testing, increase for better training\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(len(sequences) - SEQ_LENGTH):\n",
        "    seq_in = sequences[i:i+SEQ_LENGTH]   # Sequence of tokens\n",
        "    seq_out = sequences[i+SEQ_LENGTH]    # Next token\n",
        "\n",
        "    X_train.append(seq_in)\n",
        "    y_train.append(seq_out)\n",
        "\n",
        "# 📌 Convert to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# 📌 One-hot encode output labels\n",
        "y_train = to_categorical(y_train, num_classes=len(tokenizer.word_index) + 1)  # +1 for zero-indexing\n",
        "\n",
        "# 📌 Reshape input for CNN: [samples, time steps, features]\n",
        "X_train = X_train.reshape((X_train.shape[0], SEQ_LENGTH, 1))\n",
        "\n",
        "# 📌 Define the function to build the CNN model with ResNet and Strides\n",
        "def residual_block(x, filters, kernel_size, strides=1):\n",
        "    \"\"\"Residual block with Conv1D, BatchNorm, and LeakyReLU\"\"\"\n",
        "    shortcut = x\n",
        "\n",
        "    # First conv layer\n",
        "    y = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "\n",
        "    # Second conv layer\n",
        "    y = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same')(y)  # Keep spatial size\n",
        "    y = BatchNormalization()(y)\n",
        "\n",
        "    # Adjust shortcut if needed\n",
        "    if x.shape[-1] != filters or strides > 1:\n",
        "        shortcut = Conv1D(filters=filters, kernel_size=1, strides=strides, padding='same')(x)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    # Merge and activate\n",
        "    y = Add()([shortcut, y])\n",
        "    y = LeakyReLU(alpha=0.2)(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def build_resnet_model(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    x = residual_block(input_layer, filters=32, kernel_size=3, strides=2)\n",
        "    x = residual_block(x, filters=64, kernel_size=3, strides=2)\n",
        "    x = residual_block(x, filters=128, kernel_size=3, strides=2)\n",
        "    x = residual_block(x, filters=256, kernel_size=3, strides=2)\n",
        "\n",
        "    x = Flatten()(x)  # Better than Flatten for generalization\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 📌 Build and train the model\n",
        "model = build_resnet_model(input_shape=(SEQ_LENGTH, 1), num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "# 📌 Train model (increase epochs for better results)\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=256, verbose=1)\n",
        "\n",
        "# 📌 Function to generate continuous text\n",
        "def generate_text(start_sequence, num_tokens=200):\n",
        "    sequence = tokenizer.texts_to_sequences([start_sequence.lower()])[0]\n",
        "    if len(sequence) < SEQ_LENGTH:\n",
        "        return \"Input too short!\"\n",
        "\n",
        "    generated_text = start_sequence\n",
        "    for _ in range(num_tokens):\n",
        "        X_test = np.array([sequence[-SEQ_LENGTH:]]).reshape((1, SEQ_LENGTH, 1))\n",
        "        prediction = model.predict(X_test, verbose=0)\n",
        "        next_token = np.argmax(prediction)\n",
        "        next_word = tokenizer.index_word.get(next_token, '')  # Retrieve the word from token index\n",
        "        generated_text += ' ' + next_word\n",
        "        sequence.append(next_token)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# 📌 Example usage\n",
        "start_text = conversation_text[-SEQ_LENGTH-12:]  # Use the last part of the conversation\n",
        "predicted_text = generate_text(start_text, num_tokens=200)\n",
        "print(f\"Generated text:\\n{predicted_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNo2e7pyyYG2",
        "outputId": "eb5e53b2-9798-4c00-8171-4009f12426c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length: 277916\n",
            "Epoch 1/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 418ms/step - accuracy: 0.0836 - loss: 7.0909\n",
            "Epoch 2/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 410ms/step - accuracy: 0.1035 - loss: 6.3951\n",
            "Epoch 3/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 419ms/step - accuracy: 0.1045 - loss: 6.1508\n",
            "Epoch 4/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 426ms/step - accuracy: 0.1070 - loss: 5.8137\n",
            "Epoch 5/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 413ms/step - accuracy: 0.1075 - loss: 5.3637\n",
            "Epoch 6/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 407ms/step - accuracy: 0.1256 - loss: 4.7275\n",
            "Epoch 7/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 413ms/step - accuracy: 0.2281 - loss: 3.9695\n",
            "Epoch 8/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 409ms/step - accuracy: 0.2948 - loss: 3.4643\n",
            "Epoch 9/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 414ms/step - accuracy: 0.3477 - loss: 3.1197\n",
            "Epoch 10/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 406ms/step - accuracy: 0.3820 - loss: 2.8390\n",
            "Epoch 11/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 412ms/step - accuracy: 0.4146 - loss: 2.6083\n",
            "Epoch 12/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 412ms/step - accuracy: 0.4478 - loss: 2.4201\n",
            "Epoch 13/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 414ms/step - accuracy: 0.4711 - loss: 2.2657\n",
            "Epoch 14/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 408ms/step - accuracy: 0.4972 - loss: 2.0875\n",
            "Epoch 15/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 414ms/step - accuracy: 0.5259 - loss: 1.9382\n",
            "Epoch 16/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 412ms/step - accuracy: 0.5547 - loss: 1.7667\n",
            "Epoch 17/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 414ms/step - accuracy: 0.5796 - loss: 1.6385\n",
            "Epoch 18/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 409ms/step - accuracy: 0.6096 - loss: 1.4984\n",
            "Epoch 19/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 417ms/step - accuracy: 0.6420 - loss: 1.3628\n",
            "Epoch 20/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 412ms/step - accuracy: 0.6683 - loss: 1.2425\n",
            "Epoch 21/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 412ms/step - accuracy: 0.6938 - loss: 1.1175\n",
            "Epoch 22/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 412ms/step - accuracy: 0.7212 - loss: 1.0072\n",
            "Epoch 23/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 413ms/step - accuracy: 0.7555 - loss: 0.8872\n",
            "Epoch 24/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 409ms/step - accuracy: 0.7809 - loss: 0.7956\n",
            "Epoch 25/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 411ms/step - accuracy: 0.8000 - loss: 0.7053\n",
            "Epoch 26/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 407ms/step - accuracy: 0.8287 - loss: 0.6073\n",
            "Epoch 27/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 412ms/step - accuracy: 0.8566 - loss: 0.5089\n",
            "Epoch 28/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 408ms/step - accuracy: 0.8815 - loss: 0.4337\n",
            "Epoch 29/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 412ms/step - accuracy: 0.8891 - loss: 0.3967\n",
            "Epoch 30/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 411ms/step - accuracy: 0.9042 - loss: 0.3419\n",
            "Epoch 31/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 412ms/step - accuracy: 0.9198 - loss: 0.2926\n",
            "Epoch 32/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 414ms/step - accuracy: 0.9257 - loss: 0.2648\n",
            "Epoch 33/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 409ms/step - accuracy: 0.9414 - loss: 0.2193\n",
            "Epoch 34/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 414ms/step - accuracy: 0.9559 - loss: 0.1689\n",
            "Epoch 35/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 408ms/step - accuracy: 0.9625 - loss: 0.1456\n",
            "Epoch 36/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 415ms/step - accuracy: 0.9565 - loss: 0.1563\n",
            "Epoch 37/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 407ms/step - accuracy: 0.9365 - loss: 0.2092\n",
            "Epoch 38/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 414ms/step - accuracy: 0.9073 - loss: 0.2909\n",
            "Epoch 39/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 407ms/step - accuracy: 0.9199 - loss: 0.2583\n",
            "Epoch 40/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 414ms/step - accuracy: 0.9556 - loss: 0.1490\n",
            "Epoch 41/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 414ms/step - accuracy: 0.9814 - loss: 0.0793\n",
            "Epoch 42/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 416ms/step - accuracy: 0.9963 - loss: 0.0301\n",
            "Epoch 43/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 410ms/step - accuracy: 0.9998 - loss: 0.0102\n",
            "Epoch 44/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 413ms/step - accuracy: 1.0000 - loss: 0.0045\n",
            "Epoch 45/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 406ms/step - accuracy: 1.0000 - loss: 0.0030\n",
            "Epoch 46/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 0.0024\n",
            "Epoch 47/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 405ms/step - accuracy: 1.0000 - loss: 0.0021\n",
            "Epoch 48/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 412ms/step - accuracy: 1.0000 - loss: 0.0018\n",
            "Epoch 49/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 408ms/step - accuracy: 1.0000 - loss: 0.0017\n",
            "Epoch 50/50\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 411ms/step - accuracy: 1.0000 - loss: 0.0014\n",
            "Generated text:\n",
            "Input too short!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=15, batch_size=256, verbose=1)\n",
        "\n",
        "# save model\n",
        "model.save('model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1TpffW-aLeA",
        "outputId": "e926c730-eb20-492c-b854-57cea626fa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 413ms/step - accuracy: 1.0000 - loss: 0.0013\n",
            "Epoch 2/15\n",
            "\u001b[1m 63/175\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 404ms/step - accuracy: 1.0000 - loss: 0.0011"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Example usage\n",
        "print(SEQ_LENGTH)\n",
        "start_text = conversation_text[-SEQ_LENGTH-800:]\n",
        "print(start_text)\n",
        "print(len(start_text))\n",
        "predicted_text = generate_text(start_text, num_tokens=400)\n",
        "print(f\"Generated text:\\n{predicted_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIvosloK2G-f",
        "outputId": "9ab51919-69fd-491c-b47a-ed03b6fc484c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "oject Gutenberg™ electronic works\n",
            "\n",
            "Professor Michael S. Hart was the originator of the Project\n",
            "Gutenberg™ concept of a library of electronic works that could be\n",
            "freely shared with anyone. For forty years, he produced and\n",
            "distributed Project Gutenberg™ eBooks with only a loose network of\n",
            "volunteer support.\n",
            "\n",
            "Project Gutenberg™ eBooks are often created from several printed\n",
            "editions, all of which are confirmed as not protected by copyright in\n",
            "the U.S. unless a copyright notice is included. Thus, we do not\n",
            "necessarily keep eBooks in compliance with any particular paper\n",
            "edition.\n",
            "\n",
            "Most people start at our website which has the main PG search\n",
            "facility: www.gutenberg.org.\n",
            "\n",
            "This website includes information about Project Gutenberg™,\n",
            "including how to make donations to the Project Gutenberg Literary\n",
            "Archive Foundation, how to help produce our new eBooks, and how to\n",
            "subscribe to our email newsletter to hear about new eBooks.\n",
            "\n",
            "\n",
            "\n",
            "928\n",
            "Generated text:\n",
            "oject Gutenberg™ electronic works\n",
            "\n",
            "Professor Michael S. Hart was the originator of the Project\n",
            "Gutenberg™ concept of a library of electronic works that could be\n",
            "freely shared with anyone. For forty years, he produced and\n",
            "distributed Project Gutenberg™ eBooks with only a loose network of\n",
            "volunteer support.\n",
            "\n",
            "Project Gutenberg™ eBooks are often created from several printed\n",
            "editions, all of which are confirmed as not protected by copyright in\n",
            "the U.S. unless a copyright notice is included. Thus, we do not\n",
            "necessarily keep eBooks in compliance with any particular paper\n",
            "edition.\n",
            "\n",
            "Most people start at our website which has the main PG search\n",
            "facility: www.gutenberg.org.\n",
            "\n",
            "This website includes information about Project Gutenberg™,\n",
            "including how to make donations to the Project Gutenberg Literary\n",
            "Archive Foundation, how to help produce our new eBooks, and how to\n",
            "subscribe to our email newsletter to hear about new eBooks.\n",
            "\n",
            "\n",
            " of the the legbones for the harmony supination associated you this thebody of the you the respect the blood of the same movements of wound gutenberg arm the hand a c outer bone they the skin fig the strictly straight of the will the to rete length to so 8 bones as rests is the skin of its it a wristbones of terms its the the the outer from foot 44 connected by as skin the the beautiful bone the the a the the the may the tendons of ofthe the is toes the thumb a ground sole as and the thumb and isthe from the important and by part the a the sole and seen its bone skin feet metacarpal upon the feet from sole ball of the same of feet feet of the the backwards 103 at the ball at the the foot feet from ball at sole at backwards of the ground toe sole at feet of at the bones the sole of the the the sole at sole at the ball g from the foot sole 47 from the feet 47 the feet backwards g to a sole of feet sole at toe sole from sole sole 103 through of sole g g of sole of the the sole ofthe of the lowest 103 of the the the sole that is toe of bones bones of feet the bones the sole 103 as the the foot the the the foot the bones feet 14 at the feet the the run g g of the the the the thumb an the great feet as the ball muscle ligament ball at toe g of the the sole g from the inner ball g from run 103 view of toe of some bones a long g in the inner and ground in ground two great foot a sole at of 49 and a feet in exceptions from a foot g 103 36 a it 103 from of foot foot foot bones and feet thatit as may the the neither are the the bones the cleavinga showing of thrown of the wrist oilglands from direction deal of the foot rests of the sole are sole 49 from the little the the the feet together from the balls as the thumb range the the forearm muscle to the the inner the the run mentioned backwards 47 the the inner in a\n"
          ]
        }
      ]
    }
  ]
}